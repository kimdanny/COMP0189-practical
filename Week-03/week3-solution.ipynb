{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP0189: Applied Artificial Intelligence\n",
    "## Week 3 (Model Selection and Assessment)\n",
    "\n",
    "### After this week you will be able to ...\n",
    "- encode categorical values with one-hot encoding\n",
    "- know which encoding, scaling, and imputing method you should select in accordacne with the dataset characteristics\n",
    "- impute missing data with KNN\n",
    "- know how to streamline the preprocessing steps in advanced way (Pipeline and ColmnTransformer)\n",
    "- select best model based on various cross-validation methods\n",
    "\n",
    "### Acknowledgements\n",
    "- https://scikit-learn.org/stable/\n",
    "- https://archive.ics.uci.edu/ml/datasets/adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optional: For black-style code formatting within the notebook env.\n",
    "!pip install nb-black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: For black-style code formatting within the notebook env.\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previously, we used version 1.1.3 to use boston dataset as an example.\n",
    "# Since there has been many updates in core functions after version 1.2,\n",
    "# we are updating the scikit-learn to the latest version.\n",
    "# Otherwise you will get some errors and FutureWarnings.\n",
    "!pip install scikit-learn==1.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Encoding and Imputations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Load and Split the Dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TASK 1: Load Dataset\n",
    "# We are going to use the same adult dataset as previous week.\n",
    "# We have cleaned the dataset, but did not touch the missing values.\n",
    "df = pd.read_csv(\"../data/clean_adult.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(df[\"Education-num\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[\"Education\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that Education-num is just a label-encoded feature of Education\n",
    "edu_map = {}\n",
    "for i, row in df[[\"Education\", \"Education-num\"]].iterrows():\n",
    "    education = row[\"Education\"]\n",
    "    edu_num = row[\"Education-num\"]\n",
    "\n",
    "    if education not in edu_map:\n",
    "        edu_map.update({education: edu_num})\n",
    "    else:\n",
    "        assert edu_map[education] == edu_num\n",
    "edu_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's just drop Education column as we don't need it if we have Education-num\n",
    "df = df.drop([\"Education\"], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_df(df, test_ratio=0.1, target_col=\"Y\"):\n",
    "    df_data = df.drop([target_col], axis=1)\n",
    "    df_target = df[target_col]\n",
    "\n",
    "    split_index = int(len(df) * (1 - test_ratio))\n",
    "    print(f\"Splitting from index {split_index}\")\n",
    "\n",
    "    train_X_df = df_data[:split_index]\n",
    "    test_X_df = df_data[split_index:]\n",
    "    train_y_df = df_target[:split_index]\n",
    "    test_y_df = df_target[split_index:]\n",
    "\n",
    "    train_y_df = np.where(train_y_df == \">50K\", 1, 0)\n",
    "    test_y_df = np.where(test_y_df == \">50K\", 1, 0)\n",
    "\n",
    "    return train_X_df, train_y_df, test_X_df, test_y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dataset into train and test\n",
    "train_X_df, train_y_df, test_X_df, test_y_df = train_test_split_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for educational purpose:\n",
    "# For this dataset split, you can see that\n",
    "# all the unique values of categorical features in test set are subset of that in train set.\n",
    "# However, we shouldn't expect this, thus need to deal with this situation (dealt in later cells).\n",
    "for col in train_X_df:\n",
    "    # Skips for the non-categorical values\n",
    "    if col in [\n",
    "        \"Age\",\n",
    "        \"Fnlwgt\",\n",
    "        \"Capital-gain\",\n",
    "        \"Capital-loss\",\n",
    "        \"Hours-per-week\",\n",
    "    ]:\n",
    "        continue\n",
    "    train_uniq_vals = train_X_df[col].unique()\n",
    "    test_uniq_vals = test_X_df[col].unique()\n",
    "\n",
    "    if not set(test_uniq_vals).issubset(set(train_uniq_vals)):\n",
    "        print(col)\n",
    "        print(set(test_uniq_vals))\n",
    "        print(set(train_uniq_vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Encode categorical variables (label/ordinal encoding & one-hot encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important: We need special care when we are encoding categorical variables \n",
    "\n",
    "**1. Take care of the missing values**\n",
    "- Beware not to encode missing values unless you are intending to do so.\n",
    "- Sometimes you want to encode missing values to a separate cateogory. For example, when you want to predict if passengers of titanic had survived or not, missing data of certain features can actually have meaning, i.e., Cabin information can be missing because the body was not found.\n",
    "\n",
    "**2. Know which encoding and scaling method you should select**\n",
    "- If your categories are ordinal, then it makes sense to use a LabelEncoder with a MinMaxScaler. For example, you can encode [low, medium, high], as [1,2,3], i.e., distance between low to high is larger than that of medium and high.\n",
    "\n",
    "- However, if you have non-ordinal categorical values, like [White, Hispanic, Black, Asian], then it would be better to use a OneHotEncoder instead of forcing ordinality with a LabelEncoder. Otherwise the algorithms you use (especially distance based algorithms like KNN) will make the assumption that the distance between White and Asian is larger than White and Hispanic, which is nonsensical.\n",
    "\n",
    "**3. Split before you encode to avoid data leakage**\n",
    "- Split the dataset before you encode your data. It is natural for algorithms to see unknown values in the validation/test set that was not appearing in the train set. `sklearn.preprocessing.OneHotEncoder` is good at handling these unknown categories (`handle_unknown` parameter).\n",
    "\n",
    "- Discussion: What if you are certain about all the possible categories that can appear for each feature? Can you encode all the values before splitting the dataset into train and test set?\n",
    "\n",
    "\n",
    "This notebook shows the three points in the following sections with examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2-1: Label Encoding (with missing values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_df = train_X_df.isnull()\n",
    "# We will encode these columns with LabelEncoder, and the rest with OneHotEncoder\n",
    "categ = [\"Sex\"]\n",
    "train_X_df[categ] = train_X_df[categ].apply(label_encoder.fit_transform)\n",
    "# This masking process won't give any effect in this cell, where 'SEX' and 'Y' don't have any missing values.\n",
    "# However, if the columns we are encoding have missing values,\n",
    "# this is how you avoid encoding missing values as a separate category.\n",
    "# To see the effect of masking, add 'Workclass' in the categ variable and check df.isnull().sum().\n",
    "train_X_df = train_X_df.mask(mask_df, np.nan)\n",
    "train_X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The missing values are intact and were not encoded.\n",
    "train_X_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_df[\"Race\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2-2: One Hot Encoding (with missing values imputation)\n",
    "\n",
    "Tip 1: Impute the missing values (choose the right strategy) before doing OHE  \n",
    "Tip 2: Try creating a separate dataframe with one-hot encoded columns and combine the dataframe with the original dataframe for the final one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first impute the missing values.\n",
    "# Since it's a categorical value, we don't use KNN or mean imputation.\n",
    "# We will replace with the most frequent value.\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "imputed_train_X = imputer.fit_transform(train_X_df)\n",
    "imputed_train_X_df = pd.DataFrame(imputed_train_X, columns=train_X_df.columns)\n",
    "\n",
    "# Check that we have no missing values now\n",
    "imputed_train_X_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We want to turn these features into one-hot vectors\n",
    "onehot_categ = [\n",
    "    \"Workclass\",\n",
    "    \"Marital-status\",\n",
    "    \"Occupation\",\n",
    "    \"Relationship\",\n",
    "    \"Race\",\n",
    "    \"Native-country\",\n",
    "]\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False).fit(\n",
    "    imputed_train_X_df[onehot_categ]\n",
    ")\n",
    "encoded = onehot_encoder.transform(imputed_train_X_df[onehot_categ])\n",
    "encoded_df = pd.DataFrame(encoded, columns=onehot_encoder.get_feature_names_out())\n",
    "encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# After finishing encoding categorical variables,\n",
    "# we make the final dataframe by concatenating it with the imputed dataframe\n",
    "imputed_train_X_df = imputed_train_X_df.drop(onehot_categ, axis=1)\n",
    "final_df_train_X_df = pd.concat([imputed_train_X_df, encoded_df], axis=1)\n",
    "final_df_train_X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_train_X_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another (recommended) Method: Using `Pipeline` and `ColmnTransformer` to streamline the workflow.\n",
    "\n",
    "We will do the same operation (with addition of scaling) but with much more succinct way.   \n",
    "We use `Pipeline` and `ColmnTransformer`.  \n",
    "`ColmnTransformer` is needed to apply different preprocessing steps to selected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the dataset\n",
    "df = pd.read_csv(\"../data/clean_adult.csv\")\n",
    "df = df.drop([\"Education\"], axis=1)\n",
    "train_X, train_y, test_X, test_y = train_test_split_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_categorical_features = [\n",
    "    \"Age\",\n",
    "    \"Fnlwgt\",\n",
    "    \"Capital-gain\",\n",
    "    \"Capital-loss\",\n",
    "    \"Hours-per-week\",\n",
    "]\n",
    "categorical_ohe_features = [\n",
    "    \"Workclass\",\n",
    "    \"Education-num\",\n",
    "    \"Marital-status\",\n",
    "    \"Occupation\",\n",
    "    \"Relationship\",\n",
    "    \"Race\",\n",
    "    \"Native-country\",\n",
    "]\n",
    "categorical_le_features = [\"Sex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For features like 'Age' and 'Fnlwgt'\n",
    "non_categorical_transformer = Pipeline(\n",
    "    # For KNNImputer, see the side note below\n",
    "    # We can add scaling for non-categorical features\n",
    "    steps=[(\"KNNImputer\", KNNImputer(n_neighbors=5)), (\"scaling\", StandardScaler())]\n",
    ")\n",
    "\n",
    "# For features like 'Workclass' and 'Education'\n",
    "categorical_ohe_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"SimpleImputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"OHE\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")),\n",
    "        # no need to scale\n",
    "    ]\n",
    ")\n",
    "\n",
    "# For features like 'Sex'\n",
    "categorical_le_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"ModeImputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        # Here, we change to the OrdinalEncoder as LabelEncoder is used for the target variable\n",
    "        # Try changing the OrdinalEncoder to LabelEncoder to see what error you see,\n",
    "        # and check the documentation of LabelEncoder\n",
    "        (\"LE\", OrdinalEncoder()),\n",
    "        # In the case of adult dataset, no need to scale for just 'Sex' variable,\n",
    "        # but other label/ordinal encoded categorical features can be ordinal -> scaling\n",
    "        (\"scaling\", StandardScaler()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"non-categorical\", non_categorical_transformer, non_categorical_features),\n",
    "        (\"categorical-ohe\", categorical_ohe_transformer, categorical_ohe_features),\n",
    "        (\"categorical-le\", categorical_le_transformer, categorical_le_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_X = ct.fit_transform(train_X, train_y)\n",
    "transformed_test_X = ct.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_X.shape, train_y.shape, transformed_test_X.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side Note: Data Imputation with KNN\n",
    "For the adult dataset, missing data present only in categorical values, so imputing strategy that makes floating point may not make sense.\n",
    "However, for continuous values, you can use various imputation strategies, such as taking simple mean or mean value from K nearest neighbors (KNN).\n",
    "If you use `sklearn.imput.KNNImputer`, each sampleâ€™s missing values are imputed using the `mean` value from `n_neighbors` nearest neighbors found in the training set.\n",
    "If you want to use `mode` value from neighbors (for categorical data imputation) you need to implement the imputer by yourself.\n",
    "\n",
    "- `sklearn-pandas` package (https://pypi.org/project/sklearn-pandas/1.5.0/) provides `CategoricalImputer` class, which is suitable for such processing\n",
    "\n",
    "Here, we use iris dataset to show how to use KNNImputer for continuous values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying a random mask to make missing data\n",
    "mask = np.random.choice([True, False], size=iris_df.shape[0] * iris_df.shape[1])\n",
    "mask[:500] = True\n",
    "np.random.shuffle(mask)\n",
    "mask = np.reshape(mask, iris_df.shape)\n",
    "iris_df = iris_df.mask(~mask)\n",
    "\n",
    "iris_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X = iris_df[:100], iris_df[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is important to impute the train and test set separately (not fitting KNN to test set) to avoid data leak.\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "imputed_train_X = imputer.fit_transform(train_X)\n",
    "imputed_test_X = imputer.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del iris, iris_df, mask, train_X, test_X, imputer, imputed_train_X, imputed_test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Create different preprocessing strategies of your own\n",
    "Create different versions of X (X1 and X2) and y using different strategies for data imputation.  \n",
    "Define different preprocessing strategies using the `Pipeline` and `ColmnTransformer` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your explorations here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: \n",
    "Train different models (KNN, SVM) to predict the y from the two versions of X (X1 and X2) with a fixed value of the regularization parameter. \n",
    "Centre and scale the data before training the models. Create tables or plots to show how accuracy varies for different imputation strategies or different models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4-1: Training KNN and SVM Models (with original preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn = KNeighborsClassifier()\n",
    "model_knn.fit(transformed_train_X, train_y)\n",
    "\n",
    "# Predict Output\n",
    "y_hat_knn = model_knn.predict(transformed_test_X)\n",
    "accuracy_score(y_hat_knn, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm = svm.SVC(kernel=\"linear\")  # Linear Kernel\n",
    "model_svm.fit(transformed_train_X, train_y)\n",
    "\n",
    "# Predict Output\n",
    "y_hat_svm = model_svm.predict(transformed_test_X)\n",
    "accuracy_score(y_hat_svm, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4-2: Investigation\n",
    "Create tables or plots to show how accuracy varies for different imputation strategies or different models. \n",
    "- What is the impact of the imputation strategy on the accuracy? \n",
    "- What is the impact of the model on the accuracy? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Cross Validation (CV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit-learn` provides a nice visulaisation of various cross validation methods.  \n",
    "This notebook focuses on the three main CV techniques: `KFold`, `StratifiedKFold`, `GroupKFold`, and `StratifiedGroupKFold` for optimizing models' hyper-parameters and to understand how different strategies might affect the models' performance. \n",
    "\n",
    "Visit: https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#visualizing-cross-validation-behavior-in-scikit-learn\n",
    "\n",
    "![kfold](https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_006.png)\n",
    "![stra-kfold](https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_003.png)\n",
    "![group-kfold](https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_004.png)\n",
    "![stra-group-kfold](https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_010.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    StratifiedKFold,\n",
    "    GroupKFold,\n",
    "    StratifiedGroupKFold,\n",
    "    GridSearchCV,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "Now do CV to optimize the models hyperparameter using different k (2, 5, 10, 20) for the k-fold CV (KNN, SVM) to predict the y from the two versions of X (X1 and X2) and plot the model performance (mean accuracy and SD) for the different k. Centre and scale the data before training the models. \n",
    "\n",
    "Note: remember that the pre-processing steps, including data centering and scaling should be embedded in the CV. \n",
    "\n",
    "- How does the accuracy vary as the number of folds increase? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for n in [3,5,10,20]:\n",
    "    kf = KFold(n_splits=n)\n",
    "    tem = []\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(transformed_train_X)):\n",
    "        model_knn = KNeighborsClassifier()\n",
    "        k_range = np.arange(1, 10)\n",
    "        # define grid search\n",
    "        grid = dict(n_neighbors=k_range)\n",
    "        cv = KFold(n_splits=n-1)\n",
    "        grid_search = GridSearchCV(\n",
    "        estimator=model_knn,\n",
    "        param_grid=grid,\n",
    "        pre_dispatch = 6,\n",
    "        n_jobs=6,\n",
    "        cv=cv,\n",
    "        scoring=\"accuracy\",\n",
    "        error_score=0,\n",
    "        verbose=1,\n",
    "        )\n",
    "        grid_result = grid_search.fit(transformed_train_X[train_index], train_y[train_index])\n",
    "        model_knn = KNeighborsClassifier(n_neighbors = grid_result.best_params_[\"n_neighbors\"])\n",
    "        model_knn.fit(transformed_train_X[train_index], train_y[train_index])\n",
    "\n",
    "\n",
    "        y_hat_knn = model_knn.predict(transformed_train_X[test_index])\n",
    "        tem.append(accuracy_score(y_hat_knn, train_y[test_index]))\n",
    "    result.append(tem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for n in [3,5,10,20]:\n",
    "    kf = KFold(n_splits=n)\n",
    "    tem = []\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(transformed_train_X)):\n",
    "        model_svm = svm.SVC(kernel=\"linear\")\n",
    "        C_list = [0.1, 0.5, 1, 5]\n",
    "        # define grid search\n",
    "        grid = dict(C=C_list)\n",
    "        cv = KFold(n_splits=n-1)\n",
    "        grid_search = GridSearchCV(\n",
    "        estimator=model_svm,\n",
    "        param_grid=grid,\n",
    "        pre_dispatch = 16,\n",
    "        n_jobs=16,\n",
    "        cv=cv,\n",
    "        scoring=\"accuracy\",\n",
    "        error_score=0,\n",
    "        verbose=1,\n",
    "        )\n",
    "        grid_result = grid_search.fit(transformed_train_X[train_index], train_y[train_index])\n",
    "        model_svm = svm.SVC(kernel=\"linear\",C=grid_result.best_params_[\"C\"])\n",
    "        model_svm.fit(transformed_train_X[train_index], train_y[train_index])\n",
    "        y_hat_svm = model_svm.predict(transformed_train_X[test_index])\n",
    "        tem.append(accuracy_score(y_hat_svm, train_y[test_index]))\n",
    "    result.append(tem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "Repeat task 5 using stratified CV with k=5. Compute the accuracy of the models implemented in task 5 (with k=5) and check if the model with stratified CV performs better across the different folds. Centre and scale the data before training the models. Create tables or plots to show these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "kf = StratifiedKFold(n_splits=n)\n",
    "tem = []\n",
    "for i, (train_index, test_index) in enumerate(kf.split(transformed_train_X, train_y)):\n",
    "    model_knn = KNeighborsClassifier()\n",
    "    k_range = np.arange(1, 10)\n",
    "    # define grid search\n",
    "    grid = dict(n_neighbors=k_range)\n",
    "    cv = StratifiedKFold(n_splits=n - 1)\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model_knn,\n",
    "        param_grid=grid,\n",
    "        n_jobs=2,\n",
    "        cv=cv,\n",
    "        scoring=\"accuracy\",\n",
    "        error_score=0,\n",
    "        verbose=1,\n",
    "    )\n",
    "    grid_result = grid_search.fit(\n",
    "        transformed_train_X[train_index], train_y[train_index]\n",
    "    )\n",
    "    model_knn = KNeighborsClassifier(\n",
    "        n_neighbors=grid_result.best_params_[\"n_neighbors\"]\n",
    "    )\n",
    "    model_knn.fit(transformed_train_X[train_index], train_y[train_index])\n",
    "\n",
    "    y_hat_knn = model_knn.predict(transformed_train_X[test_index])\n",
    "    tem.append(accuracy_score(y_hat_knn, train_y[test_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=n)\n",
    "tem = []\n",
    "for i, (train_index, test_index) in enumerate(kf.split(transformed_train_X, train_y)):\n",
    "    model_svm = svm.SVC(kernel=\"linear\")\n",
    "    C_list = [0.1, 0.5, 1, 5]\n",
    "    # define grid search\n",
    "    grid = dict(C=C_list)\n",
    "    cv = StratifiedKFold(n_splits=n - 1)\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model_svm,\n",
    "        param_grid=grid,\n",
    "        n_jobs=2,\n",
    "        cv=cv,\n",
    "        scoring=\"accuracy\",\n",
    "        error_score=0,\n",
    "        verbose=1,\n",
    "    )\n",
    "    grid_result = grid_search.fit(\n",
    "        transformed_train_X[train_index], train_y[train_index]\n",
    "    )\n",
    "    model_svm = svm.SVC(kernel=\"linear\", C=grid_result.best_params_[\"C\"])\n",
    "    model_svm.fit(transformed_train_X[train_index], train_y[train_index])\n",
    "    y_hat_svm = model_svm.predict(transformed_train_X[test_index])\n",
    "    tem.append(accuracy_score(y_hat_svm, train_y[test_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "Repeat task 5 using stratified group CV considering 'Race' as a group with k=5.\n",
    "Compute the accuracy of the models implemented in task 5 (with k=5) and check if the model with stratified group CV performs better across the different races. \n",
    "Centre and scale the data before training the models. \n",
    "Create tables or plots to show these results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "kf = StratifiedGroupKFold(n_splits=n)\n",
    "tem = []\n",
    "group = train_X_df[\"Race\"]\n",
    "for i, (train_index, test_index) in enumerate(\n",
    "    kf.split(transformed_train_X, train_y, group)\n",
    "):\n",
    "    model_knn = KNeighborsClassifier()\n",
    "    k_range = np.arange(1, 10)\n",
    "    # define grid search\n",
    "    grid = dict(n_neighbors=k_range)\n",
    "    tem_group = train_X_df[\"Race\"][train_index]\n",
    "    cv = StratifiedGroupKFold(n_splits=n - 1)\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model_knn,\n",
    "        param_grid=grid,\n",
    "        n_jobs=2,\n",
    "        cv=cv,\n",
    "        scoring=\"accuracy\",\n",
    "        error_score=0,\n",
    "        verbose=1,\n",
    "    )\n",
    "    grid_result = grid_search.fit(\n",
    "        transformed_train_X[train_index], train_y[train_index], groups=tem_group\n",
    "    )\n",
    "    model_knn = KNeighborsClassifier(\n",
    "        n_neighbors=grid_result.best_params_[\"n_neighbors\"]\n",
    "    )\n",
    "    model_knn.fit(transformed_train_X[train_index], train_y[train_index])\n",
    "\n",
    "    y_hat_knn = model_knn.predict(transformed_train_X[test_index])\n",
    "    tem.append(accuracy_score(y_hat_knn, train_y[test_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedGroupKFold(n_splits=n)\n",
    "tem = []\n",
    "group = train_X_df[\"Race\"]\n",
    "for i, (train_index, test_index) in enumerate(kf.split(transformed_train_X, train_y,group)):\n",
    "    model_svm = svm.SVC(kernel=\"linear\")\n",
    "    C_list = [0.1, 0.5, 1, 5]\n",
    "    # define grid search\n",
    "    grid = dict(C=C_list)\n",
    "    tem_group = train_X_df[\"Race\"][train_index]\n",
    "    cv = StratifiedGroupKFold(n_splits=n - 1)\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model_svm,\n",
    "        param_grid=grid,\n",
    "        n_jobs=2,\n",
    "        cv=cv,\n",
    "        scoring=\"accuracy\",\n",
    "        error_score=0,\n",
    "        verbose=1,\n",
    "    )\n",
    "    grid_result = grid_search.fit(\n",
    "        transformed_train_X[train_index], train_y[train_index],groups=tem_group\n",
    "    )\n",
    "    model_svm = svm.SVC(kernel=\"linear\", C=grid_result.best_params_[\"C\"])\n",
    "    model_svm.fit(transformed_train_X[train_index], train_y[train_index])\n",
    "    y_hat_svm = model_svm.predict(transformed_train_X[test_index])\n",
    "    tem.append(accuracy_score(y_hat_svm, train_y[test_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c975486aa82b32d30c2438da9d14334177c2b4d93822b75ed42b1917e361f4e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

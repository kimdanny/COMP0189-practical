{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34034f63",
   "metadata": {},
   "source": [
    "# COMP0189: Applied Artificial Intelligence\n",
    "## Week 2 (Data Preprocessing)\n",
    "\n",
    "### After this week you will be able to ...\n",
    "- load various datasets from sklearn\n",
    "- know the importance of data scaling and preprocessing\n",
    "- know that the sensitivity various between learning algorithms\n",
    "- split the dataset into train and test set\n",
    "- know what will happen if you apply different preprocessing steps to train and test set\n",
    "- know how to encode categorical features to ordinal representation and how it affects the model performance\n",
    "- know how to deal with missing data\n",
    "\n",
    "### Acknowledgements\n",
    "- https://github.com/UCLAIS/Machine-Learning-Tutorials\n",
    "- https://www.cs.columbia.edu/~amueller/comsw4995s19/schedule/\n",
    "- https://scikit-learn.org/stable/\n",
    "- https://archive.ics.uci.edu/ml/datasets/adult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28f1108",
   "metadata": {},
   "source": [
    "## Introduction to Scikit-learn\n",
    "\n",
    "Why do we use sklearn??\n",
    "\n",
    "1. Example Datasets\n",
    "    - sklearn.datasets : Provides example datasets\n",
    "\n",
    "2. Feature Engineering  \n",
    "    - sklearn.preprocessing : Variable functions as to data preprocessing\n",
    "    - sklearn.feature_selection : Help selecting primary components in datasets\n",
    "    - sklearn.feature_extraction : Vectorised feature extraction\n",
    "    - sklearn.decomposition : Algorithms regarding Dimensionality Reduction\n",
    "\n",
    "3. Data split and Parameter Tuning  \n",
    "    - sklearn.model_selection : 'Train Test Split' for cross validation, Parameter tuning with GridSearch\n",
    "\n",
    "4. Evaluation  \n",
    "    - sklearn.metrics : accuracy score, ROC curve, F1 score, etc.\n",
    "\n",
    "5. ML Algorithms\n",
    "    - sklearn.ensemble : Ensemble, etc.\n",
    "    - sklearn.linear_model : Linear Regression, Logistic Regression, etc.\n",
    "    - sklearn.naive_bayes : Gaussian Naive Bayes classification, etc.\n",
    "    - sklearn.neighbors : Nearest Centroid classification, etc.\n",
    "    - sklearn.svm : Support Vector Machine\n",
    "    - sklearn.tree : DecisionTreeClassifier, etc.\n",
    "    - sklearn.cluster : Clustering (Unsupervised Learning)\n",
    "\n",
    "6. Utilities  \n",
    "    - sklearn.pipeline: pipeline of (feature engineering -> ML Algorithms -> Prediction)\n",
    "\n",
    "7. Train and Predict  \n",
    "    - fit()\n",
    "    - predict()\n",
    "\n",
    "8. and more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb38f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn==1.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0eca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5fdd50",
   "metadata": {},
   "source": [
    "**1. Boston House Price Dataset**\n",
    "\n",
    "Let's first take a look at the Boston House Price dataset. This Dataset is deprecated as of version 1.2, but we will use this for educational purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8783fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111eb500",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fffb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.feature_names, len(boston.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2004f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, y = boston.data, boston.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefdb9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 5, figsize=(20, 10))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    if i > 12:\n",
    "        ax.set_visible(False)\n",
    "        continue\n",
    "    ax.plot(X[:, i], y, 'o', alpha=.5)\n",
    "    ax.set_title(\"{}: {}\".format(i, boston.feature_names[i]))\n",
    "    ax.set_ylabel(\"PRICE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58cc370",
   "metadata": {},
   "source": [
    "See how our data are spread in different ranges. 3rd feature (CHAS) is even in binary. Most of the algorithms perform poorly on these various input spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af267df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e949cac4",
   "metadata": {},
   "source": [
    "**2. Wine Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936d1584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17250624",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "print(wine.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e30564c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d81aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_X = wine.data\n",
    "wine_labels = wine.target\n",
    "wine_feature_names = wine.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d26d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d5e1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(wine_X, columns=wine_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dcb7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_wine(X, labels=None, column_indices=(0,1), set_labels=False):\n",
    "    \"\"\"\n",
    "    @param: X        --> Data\n",
    "    @param: lables   --> Default is set to None, but if you've got your result of labels from clustering, \n",
    "                         you can input according labels in a list format.\n",
    "    @param: column_indices --> column indices of dataset X to be selected for plotting.\n",
    "                                 two-element tuple if you want 2D graph,\n",
    "                                 three-element tuple if you want 3D graph.\n",
    "    \"\"\"\n",
    "    assert type(column_indices) is tuple\n",
    "    \n",
    "    if len(column_indices)==2:  # 2D\n",
    "        first_col, second_col = column_indices[0], column_indices[1]\n",
    "        \n",
    "        if set_labels:\n",
    "            plt.xlabel(wine_feature_names[first_col])\n",
    "            plt.ylabel(wine_feature_names[second_col])\n",
    "            \n",
    "        plt.scatter(X[:, first_col], X[:, second_col], c=labels)\n",
    "        \n",
    "    elif len(column_indices)==3:  # 3D\n",
    "        first_col, second_col, third_col = column_indices[0], column_indices[1], column_indices[2]\n",
    "        fig = plt.figure()\n",
    "        plt.clf()\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "        plt.cla()\n",
    "        \n",
    "        if set_labels:\n",
    "            ax.set_xlabel(wine_feature_names[first_col])\n",
    "            ax.set_ylabel(wine_feature_names[second_col])\n",
    "            ax.set_zlabel(wine_feature_names[third_col])\n",
    "\n",
    "        ax.scatter(X[:, first_col], X[:, second_col], X[:, third_col], c=labels)\n",
    "        \n",
    "    else:\n",
    "        raise RuntimeError(\"Your dimension should be either set to \\\"2d\\\" or \\\"3d\\\"\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36356fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_wine(wine_X, labels=wine_labels, column_indices=(8, 10), set_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754a3fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try out different col_in_X and get some feeling of how the data is shaped.\n",
    "visualise_wine(wine_X, labels=wine_labels, column_indices=(8, 10, 12), set_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99a2680",
   "metadata": {},
   "source": [
    "We will closely look into details of many functions in scikit-learn (fit, predict, PCA, metrics, etc.) in the following practicals as we learn more in lectures.  \n",
    "For now, it is good to be familiar with datasets and the main takeaways we demonstrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee6973e",
   "metadata": {},
   "source": [
    "## Exercise 1: Impact of feature scaling in ML pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5dede2",
   "metadata": {},
   "source": [
    "Normalization scales each input variable separately to the range 0-1.  \n",
    "Standardization scales each input variable separately by subtracting the mean (centering) and dividing each of them by the standard deviation to shift the distribution to have a mean of zero and a standard deviation of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18994d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96b1bba",
   "metadata": {},
   "source": [
    "#### Examaple usage of sklearn.preprocessing.StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce7c672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "unscaled_data = np.asarray([[100, 0.001],\n",
    " [8, 0.05],\n",
    " [50, 0.005],\n",
    " [88, 0.07],\n",
    " [4, 0.1]])\n",
    "# define standard scaler\n",
    "scaler = StandardScaler()\n",
    "# transform data\n",
    "scaled_data = scaler.fit_transform(unscaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c79b31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(unscaled_data).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b7ceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scaled_data).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1244e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del scaled_data, unscaled_data, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe24e99f",
   "metadata": {},
   "source": [
    "**Questions**  \n",
    "- Try using different methods, such as MinMaxScaler and Normalisation. Do you see the difference in the histogram?\n",
    "- Experiment the effects of different feature scaling methods on one ML algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465fa5bd",
   "metadata": {},
   "source": [
    "### Scaling Vs. Unscaling the Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdfa4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "# We are using the wind dataset again\n",
    "features, target = load_wine(return_X_y=True)\n",
    "\n",
    "# Make a train/test split using 30% test size\n",
    "X_train, X_test, y_train, y_test = train_test_split(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d38b5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "unscaled_X_train = X_train\n",
    "unscaled_X_test = X_test\n",
    "\n",
    "# scale data\n",
    "scaled_X_train = scaler.None\n",
    "scaled_X_test = scaler.None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5b7f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_model = KNeighborsClassifier()\n",
    "scaled_model = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9821649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: fit each data with unscaled and scaled train set\n",
    "unscaled_model.None\n",
    "scaled_model.None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1cd747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: predict y_hat with scaled/unscaled test set\n",
    "unscaled_y_hat = unscaled_model.None\n",
    "scaled_y_hat = scaled_model.None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b066667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: using accuracy_score() get the accuracy metric of both model\n",
    "unscaled_acc = accuracy_score(y_test, unscaled_y_hat)\n",
    "scaled_acc = accuracy_score(y_test, scaled_y_hat)\n",
    "unscaled_acc, scaled_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b715b4",
   "metadata": {},
   "source": [
    "## Exercise 2: Impact of different preprocessing strategy in train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caf828b",
   "metadata": {},
   "source": [
    "Do you see the difference in accuracy?  \n",
    "**Question**  \n",
    "Now, notice that I also scaled the test set.   \n",
    "Using the same code, see what happens if you don't scale the test data and predict based on the unscaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d3b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the same test data for both unscaled and scaled model\n",
    "unscaled_y_hat = unscaled_model.predict(None)\n",
    "scaled_y_hat = scaled_model.predict(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfb476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_acc = accuracy_score(y_test, unscaled_y_hat)\n",
    "scaled_acc = accuracy_score(y_test, scaled_y_hat)\n",
    "unscaled_acc, scaled_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8567690",
   "metadata": {},
   "source": [
    "## sklearn.pipeline.make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e93b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_make_pipeline(pca_enable=False):\n",
    "    # Fit to data and predict using pipeline\n",
    "    if pca_enable:\n",
    "        unscaled_clf = make_pipeline(PCA(n_components=2), KNeighborsClassifier())\n",
    "    else:\n",
    "        unscaled_clf = make_pipeline(KNeighborsClassifier())\n",
    "    unscaled_clf.fit(X_train, y_train)\n",
    "    pred_test = unscaled_clf.predict(X_test)\n",
    "\n",
    "    # Fit to data and predict using pipeline\n",
    "    if pca_enable:\n",
    "        std_clf = make_pipeline(StandardScaler(), PCA(n_components=2), KNeighborsClassifier())\n",
    "    else:\n",
    "        std_clf = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "    std_clf.fit(X_train, y_train)\n",
    "    pred_test_std = std_clf.predict(X_test)\n",
    "\n",
    "    # Show prediction accuracies in scaled and unscaled data.\n",
    "    print(\"\\nPrediction accuracy for the normal test dataset\")\n",
    "    print(f\"{accuracy_score(y_test, pred_test):.2%}\\n\")\n",
    "\n",
    "    print(\"\\nPrediction accuracy for the standardized test dataset\")\n",
    "    print(f\"{accuracy_score(y_test, pred_test_std):.2%}\\n\")\n",
    "\n",
    "    # Extract PCA from pipeline3\n",
    "    # print(unscaled_clf.named_steps)\n",
    "    # {'pca': PCA(n_components=2), 'gaussiannb': GaussianNB()}\n",
    "    # print(std_clf.named_steps)\n",
    "    # {'standardscaler': StandardScaler(), 'pca': PCA(n_components=2), 'gaussiannb': GaussianNB()}\n",
    "\n",
    "    try:\n",
    "        pca = unscaled_clf.named_steps[\"pca\"]\n",
    "        pca_std = std_clf.named_steps[\"pca\"]\n",
    "\n",
    "        # Show first principal components\n",
    "        print(f\"\\nPC 1 without scaling:\\n{pca.components_[0]}\")\n",
    "        print(f\"\\nPC 1 with scaling:\\n{pca_std.components_[0]}\")\n",
    "\n",
    "        # Use PCA without and with scale on X_train data for visualization.\n",
    "        X_train_transformed = pca.transform(X_train)\n",
    "\n",
    "        scaler = std_clf.named_steps[\"standardscaler\"]\n",
    "        scaled_X_train = scaler.transform(X_train)\n",
    "        X_train_std_transformed = pca_std.transform(scaled_X_train)\n",
    "\n",
    "        # visualize standardized vs. untouched dataset with PCA performed\n",
    "        fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 7))\n",
    "\n",
    "        target_classes = range(0, 3)\n",
    "        colors = (\"blue\", \"red\", \"green\")\n",
    "        markers = (\"^\", \"s\", \"o\")\n",
    "\n",
    "        for target_class, color, marker in zip(target_classes, colors, markers):\n",
    "            ax1.scatter(\n",
    "                x=X_train_transformed[y_train == target_class, 0],\n",
    "                y=X_train_transformed[y_train == target_class, 1],\n",
    "                color=color,\n",
    "                label=f\"class {target_class}\",\n",
    "                alpha=0.5,\n",
    "                marker=marker,\n",
    "            )\n",
    "\n",
    "            ax2.scatter(\n",
    "                x=X_train_std_transformed[y_train == target_class, 0],\n",
    "                y=X_train_std_transformed[y_train == target_class, 1],\n",
    "                color=color,\n",
    "                label=f\"class {target_class}\",\n",
    "                alpha=0.5,\n",
    "                marker=marker,\n",
    "            )\n",
    "\n",
    "        ax1.set_title(\"Training dataset after PCA\")\n",
    "        ax2.set_title(\"Standardized training dataset after PCA\")\n",
    "\n",
    "        for ax in (ax1, ax2):\n",
    "            ax.set_xlabel(\"1st principal component\")\n",
    "            ax.set_ylabel(\"2nd principal component\")\n",
    "            ax.legend(loc=\"upper right\")\n",
    "            ax.grid()\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.show()\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aab2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_make_pipeline(pca_enable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678a4377",
   "metadata": {},
   "source": [
    "**Question**  \n",
    "Try changing the KNN algorithm with different ones, such as Gaussian Naïve Bayes and Decision Trees in the pipeline. What do you notice in the accuracy of the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f049513",
   "metadata": {},
   "source": [
    "### sneak peek to the future session (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e89e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_make_pipeline(pca_enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25847cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "232ba604",
   "metadata": {},
   "source": [
    "## Now we move on the next session which is about categorial features and data imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a61ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6977889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the csv file and skim through it. It does not have column names \n",
    "# so we will allocate names to each column \n",
    "\n",
    "# Naming the Columns\n",
    "names = ['age','workclass','fnlwgt','education','education-num',\n",
    "        'marital-status','occupation','relationship','race','sex',\n",
    "        'capital-gain','capital-loss','hours-per-week','native-country',\n",
    "        'y']\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('../data/adult.csv', names=names, na_values='?')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad2633",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99897885",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f0773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 1: Get the unique values in the race column \n",
    "df['race'].None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95cd314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 2: Get the unique values in the 'y' column \n",
    "df['y'].None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245ce71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the popluation count by race\n",
    "counts = df['race'].value_counts()\n",
    "labels = counts.index\n",
    "\n",
    "# Plot pie chart\n",
    "plt.pie(counts, startangle=90)\n",
    "plt.legend(labels, loc=2,fontsize=8)\n",
    "plt.title(\"Race\",size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c51abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 3\n",
    "# We see redundant space prefix in the values. Remove them. Hint: apply() function\n",
    "df['race'] = df['race'].None\n",
    "df['y'] = df['y'].None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a747d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['race'].unique(), df['y'].unique(), df['occupation'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ded9523",
   "metadata": {},
   "source": [
    "Hmmm it's not just the race and y column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a575e39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to apply this to all the string-valued columns\n",
    "for col_name in df.columns:\n",
    "    if not 'int' in str(df[col_name].dtype):\n",
    "        df[col_name] = df[col_name].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e919df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in df.columns:\n",
    "    if not 'int' in str(df[col_name].dtype):\n",
    "        print(df[col_name].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51257e6",
   "metadata": {},
   "source": [
    "All done!  \n",
    "Now let's specifically look into the 'race' and 'y' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec1b341",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['race', 'y']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116f91d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 4: Convert categorical and target variables to binary numerical values\n",
    "# We now show converting them into binary values, but later in this notebook \n",
    "# we will show how we encode them into different labels using LabelEncoder and OneHotEncoder\n",
    "# Also, if you see a SettingWithCopyWarning, ignore for now.\n",
    "\n",
    "# Converting White into 1 else 0\n",
    "# df_numerical['num_race'] = [1 if r=='White' else 0 for r in df_numerical['race']]\n",
    "df[\"race\"] = None\n",
    "\n",
    "# Define target variable \n",
    "# Converting >50k into 1 and others into 0\n",
    "df[\"y\"] = None\n",
    "\n",
    "df[['race', 'y']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3f7c0c",
   "metadata": {},
   "source": [
    "Now, let's map the occupation into different numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2fae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['occupation'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f09193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For occupation converting different categories to numerical values\n",
    "occ_mapping = {\n",
    "    'Priv-house-serv':0,'?':-1, 'Other-service':0,'Handlers-cleaners':0,\n",
    "    'Farming-fishing':1,'Machine-op-inspct':1,'Adm-clerical':1,\n",
    "    'Transport-moving':2,'Craft-repair':2,'Sales':2,\n",
    "    'Armed-Forces':3,'Tech-support':3,'Protective-serv':3,\n",
    "    'Prof-specialty':4,'Exec-managerial':4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee9d3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 5: using 'map' function in pandas, map the categorical values into numerical values\n",
    "df[\"occupation\"] = None\n",
    "df['occupation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897db437",
   "metadata": {},
   "source": [
    "### Dealing with Missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25831b63",
   "metadata": {},
   "source": [
    "#### In processing the data earlier, we did not take account of the missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf7cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a0c2dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TASK 7\n",
    "# Drop the missing values, i.e. values with '?' in the occupation and native country \n",
    "None\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db45aba",
   "metadata": {},
   "source": [
    "Look, the the number of rows shrinked down to 30162 from 32561"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc3d8c8",
   "metadata": {},
   "source": [
    "Above is the case where we want specific ordinal values for each occupation. What if we don't care?\n",
    "We can use LabelEncoder\n",
    "###  Basic Usage of LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879729ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['workclass'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b9a09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d9f1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's encode workclass column\n",
    "work_class_list = list(df['workclass'].unique())\n",
    "work_class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72068258",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder.fit(work_class_list)\n",
    "list(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d69621",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder.transform(work_class_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9548b6ec",
   "metadata": {},
   "source": [
    "### Now let's use this in our df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf0a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287425ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54976e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "categ = ['workclass','education','marital-status','relationship', 'sex', 'native-country']\n",
    "\n",
    "# TASK 6: Encode Categorical Columns\n",
    "# label_encoder.fit_transform fits label encoder and returns encoded labels.\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a5aaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a28f3f",
   "metadata": {},
   "source": [
    "### Now, train an SVM or KNN Classifier and check the metrics by using the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e991bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 8: Training an SVM Classifier\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6683e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcualte the Accuracy of the model\n",
    "def accuracy_metric(y, y_pred):\n",
    "    \"\"\"Calculate fairness for subgroup of population\"\"\"\n",
    "    \n",
    "    cm=confusion_matrix(y, y_pred)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    \n",
    "    N = TP+FP+FN+TN #Total population\n",
    "    ACC = (TP+TN)/N #Accuracy\n",
    "    TPR = TP/(TP+FN) # True positive rate\n",
    "    FPR = FP/(FP+TN) # False positive rate\n",
    "    FNR = FN/(TP+FN) # False negative rate\n",
    "    PPP = (TP + FP)/N # % predicted as positive\n",
    "    \n",
    "    return np.array([ACC, TPR, FPR, FNR, PPP])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7d0e0b",
   "metadata": {},
   "source": [
    "#### Question 1 : Try training a classifier with and without dealing with missing values\n",
    "#### Question 2: Try OneHotEncoder instead of LabelEncoder and compare the performance of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aabc2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

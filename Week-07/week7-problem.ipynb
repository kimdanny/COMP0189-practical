{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4660a734",
   "metadata": {},
   "source": [
    "# COMP0189: Applied Artificial Intelligence\n",
    "## Week 7 (Dimensionality reduction and clustering)\n",
    "\n",
    "Part 1: MNIST dataset  \n",
    "Part 2: OASIS dataset  \n",
    "\n",
    "### Acknowledgements\n",
    "- https://scikit-learn.org/stable/\n",
    "- https://oasis-brains.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdf27f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f55e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b18ed1",
   "metadata": {},
   "source": [
    "## Part 1: MNIST dataset: Principal Component Analysis and clustering in a latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8026c4",
   "metadata": {},
   "source": [
    "**Task 1: Load MNIST data and assemble it in two matrices X (images) and y (labels)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c691c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST = np.load(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243bcc96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07fa23e5",
   "metadata": {},
   "source": [
    "**Task 2: Visualise the data for better understanding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08bf2a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c142ec3",
   "metadata": {},
   "source": [
    "#### Pre-processing the data\n",
    "\n",
    "One challenge with K-means is that it can be slow to find the nearest cluster centers for each datapoint. This is particularly for high dimensional data (or for large K).\n",
    "\n",
    "An easy way to speed things up is to pre-process the data by reducing its dimensionality.\n",
    "\n",
    "Here, we will run PCA as pre-processing. The next cells reshape the original MNIST data, from\n",
    "\n",
    "* `mnist_images.shape == [60000, 28, 28]`\n",
    "\n",
    "into a 2d array (matrix)\n",
    "\n",
    "* `X_mnist.shape == [60000, 784]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd456e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b93f31",
   "metadata": {},
   "source": [
    "**Task 3: Apply PCA to the MNIST data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44615f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d645557e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aad1f0b4",
   "metadata": {},
   "source": [
    "**Task 4: Plot the MNIST data projected onto the first two principal components (using different colours for the different digits). Use the labels to colour the examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb54b514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bdc9771",
   "metadata": {},
   "source": [
    "**Task 5: Plot the explained variance per component. Based on the plot decide on how many components should be used for clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b62d08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c870d1c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "506440ce",
   "metadata": {},
   "source": [
    "**Task 6: Apply KMeans to PCA-reduced data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caea07d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA of 200 components\n",
    "pca = PCA(n_components=None)\n",
    "mnist_X_pca = pca.fit_transform(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53555c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform KMeans\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5805d1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "kmeans = KMeans(None)\n",
    "kmeans.fit(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c7eb08",
   "metadata": {},
   "source": [
    "**Task 7: Visualise example of the clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a57d87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0db0878",
   "metadata": {},
   "source": [
    "**Task 8: Evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a416a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# Define a function to calculate and print accuracy, precision and recall scores\n",
    "def evaluate(labels_true, labels_pred):\n",
    "    accuracy = metrics.accuracy_score(labels_true, labels_pred)\n",
    "    precision = metrics.precision_score(\n",
    "        labels_true, labels_pred, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    recall = metrics.recall_score(\n",
    "        labels_true, labels_pred, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67696103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b4c7a51",
   "metadata": {},
   "source": [
    "### `silhouette_score` and Evaluation Benchmark for  KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e0896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def bench_k_means(model, name, data, labels):\n",
    "    \"\"\"Benchmark to evaluate the KMeans initialization methods.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model\n",
    "    name : str\n",
    "        Name given to the strategy. It will be used to show the results in a\n",
    "        table.\n",
    "    data : ndarray of shape (n_samples, n_features)\n",
    "        The data to cluster.\n",
    "    labels : ndarray of shape (n_samples,)\n",
    "        The labels used to compute the clustering metrics which requires some\n",
    "        supervision.\n",
    "    \"\"\"\n",
    "    t0 = time()\n",
    "    # can add your own pipeline, but we just use our model here\n",
    "    estimator = make_pipeline(model).fit(data)\n",
    "    fit_time = time() - t0\n",
    "\n",
    "    reference_labels = retrieve_info(estimator[-1].labels_, labels)\n",
    "    number_labels = np.random.rand(len(estimator[-1].labels_))\n",
    "    for i in range(len(estimator[-1].labels_)):\n",
    "        number_labels[i] = reference_labels[estimator[-1].labels_[i]]\n",
    "\n",
    "    # inertia_: Sum of squared distances of samples to their closest cluster center,\n",
    "    #           weighted by the sample weights if provided.\n",
    "    results = [name, fit_time, estimator[-1].inertia_]\n",
    "\n",
    "    # Define the metrics which require only the true labels and estimator\n",
    "    clustering_metrics = [\n",
    "        metrics.homogeneity_score,\n",
    "        metrics.completeness_score,\n",
    "        metrics.v_measure_score,\n",
    "        metrics.adjusted_rand_score,\n",
    "        metrics.adjusted_mutual_info_score,\n",
    "    ]\n",
    "    results += [m(labels, number_labels) for m in clustering_metrics]\n",
    "\n",
    "    # The silhouette score requires the full dataset\n",
    "    results += [\n",
    "        metrics.silhouette_score(\n",
    "            data,\n",
    "            estimator[-1].labels_,\n",
    "            metric=\"euclidean\",\n",
    "            sample_size=300,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # traditional metrics with true labels\n",
    "    results += [\n",
    "        metrics.accuracy_score(labels, number_labels),\n",
    "        metrics.precision_score(\n",
    "            labels, number_labels, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        metrics.recall_score(labels, number_labels, average=\"macro\", zero_division=0),\n",
    "    ]\n",
    "\n",
    "    # Show the results\n",
    "    formatter_result = \"{:9s}\\t{:.3f}s\\t{:.0f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\"\n",
    "    print(formatter_result.format(*results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f40e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(100 * \"_\")\n",
    "print(\"model\\t\\ttime\\tinertia\\thomo\\tcompl\\tv-meas\\tARI\\tAMI\\tsilh\\taccu\\tprec\\trecall\")\n",
    "\n",
    "model = KMeans(n_clusters=n_digits, n_init=10, random_state=0)\n",
    "bench_k_means(model=model, name=\"kmeans\", data=mnist_X, labels=mnist_y)\n",
    "\n",
    "model = MiniBatchKMeans(n_clusters=n_digits, n_init=10, random_state=0)\n",
    "bench_k_means(model=model, name=\"minibatch\", data=mnist_X, labels=mnist_y)\n",
    "\n",
    "# change n_components of pca\n",
    "pca = PCA(n_components=200)\n",
    "mnist_X_pca = pca.fit_transform(mnist_X)\n",
    "model = KMeans(n_clusters=n_digits, n_init=10, random_state=0)\n",
    "bench_k_means(model=model, name=\"PCA-kmeans\", data=mnist_X_pca, labels=mnist_y)\n",
    "\n",
    "pca = PCA(n_components=200)\n",
    "mnist_X_pca = pca.fit_transform(mnist_X)\n",
    "model = MiniBatchKMeans(n_clusters=n_digits, n_init=10, random_state=0)\n",
    "bench_k_means(model=model, name=\"PCA-minibatch\", data=mnist_X_pca, labels=mnist_y)\n",
    "\n",
    "print(100 * \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdd4cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a50fcbe",
   "metadata": {
    "id": "QRnkq4-21hPC"
   },
   "source": [
    "## Part 2: OASIS dataset: Cross decomposition methods and clustering in the latent space\n",
    "\n",
    "In this part, you will learn how to apply cross decomposition methods such as CCA and PLSSVD to find the fundamental relations between two matrices (X and Y) that represent different views of the same data. You will also learn how to use KMeans clustering to group the data points based on their latent representations in a lower-dimensional space.\n",
    "\n",
    "We will use the OASIS dataset, which contains brain MRI images (view 1) and clinical assessments (view 2) of 416 subjects aged 18 to 96. The goal is to explore how these two views are related and how they can be used for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b50c4f1",
   "metadata": {
    "id": "kbqSWOnj3Ue1"
   },
   "source": [
    "## Import libraries and load data\n",
    "\n",
    "First, we need to import some libraries and load the data from CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49df5926",
   "metadata": {
    "id": "ZIT8BAUH1sKQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA, PLSSVD\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "labels = pd.read_csv(\"../data/OASIS_labels.csv\")\n",
    "brain_roi = pd.read_csv(\"../data/OASIS_view1_ROI.csv\")\n",
    "clinical = pd.read_csv(\"../data/OASIS_view2_clinical.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3723db4",
   "metadata": {
    "id": "65F-szAW3bEy"
   },
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Next, we need to do some preprocessing on the data. We will drop some columns that are not relevant for our analysis, such as subject ID, gender, handness, etc. We will also normalize each view by subtracting its mean and dividing by its standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca62b6c",
   "metadata": {
    "id": "ALxkIlHu3e5E"
   },
   "outputs": [],
   "source": [
    "# Drop irrelevant columns\n",
    "brain_roi = brain_roi.drop([\"Subject ID\"], axis=1)\n",
    "clinical = clinical.drop([\"Subject ID\"], axis=1)\n",
    "\n",
    "# Fill nans with mean values\n",
    "None\n",
    "\n",
    "# Normalize each view\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb964452",
   "metadata": {
    "id": "xgMhVNPg3ifh"
   },
   "source": [
    "## Cross decomposition methods\n",
    "\n",
    "Now we are ready to apply cross decomposition methods to find the relations between the two views. We will use two methods: CCA and PLSSVD.\n",
    "\n",
    "CCA finds linear combinations of X and Y that have maximum correlation1. It can be seen as a generalization of PCA for two sets of variables.\n",
    "\n",
    "PLSSVD finds linear combinations of X and Y that have maximum covariance. It can be seen as a generalization of SVD for two sets of variables.\n",
    "\n",
    "For both methods, we need to specify the number of components (n_components) that we want to extract from each view. This parameter controls the dimensionality of the latent space where we will cluster the data points later.\n",
    "\n",
    "We will use n_components=2 for both methods. You can try different values later and see how they affect the results.\n",
    "\n",
    "For further comparison, apply PCA to the brain_roi data with 2 components in order to see if combining the modalities improves the latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f5fefc",
   "metadata": {
    "id": "dSDAko7X3r_l"
   },
   "source": [
    "### CCA Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b96a9a",
   "metadata": {
    "id": "_tpgWgT23o6Y"
   },
   "outputs": [],
   "source": [
    "CCA_n_components = None\n",
    "# Create CCA object with n_components=2\n",
    "cca = CCA(n_components=CCA_n_components)\n",
    "\n",
    "# Fit CCA model on X (brain_roi) and Y (clinical)\n",
    "cca.fit(None)\n",
    "\n",
    "# Transform X and Y into their latent representations using CCA\n",
    "X_c, Y_c = cca.transform(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64968bb0",
   "metadata": {
    "id": "U1pWTazs6k8h"
   },
   "source": [
    "### CCA Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137885f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "id": "hV68J8Wd6mmk",
    "outputId": "e45da2b8-184e-446e-fe52-960dd1d91fba"
   },
   "outputs": [],
   "source": [
    "# Convert labels to numbers\n",
    "label_dict = {\"Demented\": 0, \"Nondemented\": 1}\n",
    "label_nums = labels[\"Group\"].map(label_dict)\n",
    "\n",
    "# Plot the latent dimensions for CCA\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba1c034",
   "metadata": {
    "id": "loCwzyga3tOl"
   },
   "source": [
    "### PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a54190",
   "metadata": {
    "id": "sVMVjhvg3raz"
   },
   "outputs": [],
   "source": [
    "PLSSVD_n_components = None\n",
    "# Create PLSSVD object with n_components=3\n",
    "plssvd = PLSSVD(n_components=PLSSVD_n_components)\n",
    "\n",
    "# Fit PLSSVD model on X (brain_roi) and Y (clinical)\n",
    "plssvd.fit(None)\n",
    "\n",
    "# Transform X and Y into their latent representations using PLSSVD\n",
    "X_p, Y_p = plssvd.transform(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcade344",
   "metadata": {
    "id": "yNLqhAyN6sEJ"
   },
   "source": [
    "### PLS Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916931c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "id": "y6lCKIFK6tif",
    "outputId": "745073d2-485e-432e-abeb-e57463bc9811"
   },
   "outputs": [],
   "source": [
    "# Plot the latent dimensions for PLSSVD\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c72a629",
   "metadata": {
    "id": "Y6u2Ofl19hJk"
   },
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53105a75",
   "metadata": {
    "id": "Nbfnaio69juj"
   },
   "outputs": [],
   "source": [
    "# Perform PCA on brain_roi\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02274bc",
   "metadata": {
    "id": "ON_HrY9x9v1K"
   },
   "source": [
    "### PCA Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a46956",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "xcqvFQiF9xLc",
    "outputId": "4d7a2d40-5602-4a21-f113-501f75829e72"
   },
   "outputs": [],
   "source": [
    "# Plot brain principal components as a scatter plot\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce41404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on clinical variables\n",
    "pca = PCA(n_components=2)\n",
    "Y_pca = pca.fit_transform(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d458e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot clinical principal components as a scatter plot\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d017b04e",
   "metadata": {
    "id": "xHz5uQWh3wTG"
   },
   "source": [
    "## Clustering in the latent space\n",
    "\n",
    "Finally, we will use KMeans clustering to group the data points based on their latent representations obtained from CCA, PLSSVD, and PCA. For CCA and PLSSVD we will average or add the X_c/Y_c and X_p/Y_p respectively while for PCA we will just use X_pca. We will use n_clusters=2 for KMeans, which corresponds to two groups: Non-Demented (ND) and Demented (D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5399a4a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQ62AXNg3_KW",
    "outputId": "1686b996-f308-4ac5-b651-b4a6cfbedf10"
   },
   "outputs": [],
   "source": [
    "# Create KMeans object with n_clusters=3\n",
    "kmeans = KMeans(n_clusters=2, n_init=10)\n",
    "\n",
    "# Cluster the data points based on their latent representations from CCA\n",
    "kmeans.fit(None)\n",
    "labels_c = kmeans.labels_\n",
    "\n",
    "# Cluster the data points based on their latent representations from PLSSVD\n",
    "kmeans.fit(None)\n",
    "labels_p = kmeans.labels_\n",
    "\n",
    "# Cluster the data points based on their latent representations from PCA brain_roi\n",
    "kmeans.fit(X_pca)\n",
    "labels_brain = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1498335",
   "metadata": {
    "id": "k4ZZkPwF8337"
   },
   "source": [
    "## Quantify Performance of K-Means classifiers using the different latent spaces\n",
    "NOTE You may need to reverse the sign on the kmeans labels if accuracy is below 0.5 for any model since k-means does not know the order of the original labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad61f19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hfkwA22V9AyY",
    "outputId": "e764ea05-5eb4-43dd-c357-16fd2c7eec69"
   },
   "outputs": [],
   "source": [
    "# Import metrics module from sklearn\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# Define a function to calculate and print accuracy, precision and recall scores\n",
    "def evaluate(labels_true, labels_pred):\n",
    "    accuracy = metrics.accuracy_score(labels_true, labels_pred)\n",
    "    precision = metrics.precision_score(labels_true, labels_pred)\n",
    "    recall = metrics.recall_score(labels_true, labels_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\\n\")\n",
    "\n",
    "\n",
    "# Compare the performance of classifiers based on CCA, PLSSVD, PCA brain_roi and PCA clinical\n",
    "# assuming true_labels is a variable that stores the ground truth labels\n",
    "print(\"Performance of classifier based on CCA:\")\n",
    "evaluate(label_nums, labels_c)\n",
    "print(\"Performance of classifier based on PLSSVD:\")\n",
    "evaluate(label_nums, labels_p)\n",
    "print(\"Performance of classifier based on PCA brain_roi:\")\n",
    "evaluate(label_nums, labels_brain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83f34a5",
   "metadata": {
    "id": "ZD9uceCd4CDJ"
   },
   "source": [
    "## Conclusion\n",
    "This week you learned how to apply cross decomposition methods such as CCA and PLSSVD to find the relations between two views of the same data. You also learned how to use KMeans clustering to group the data points based on their latent representations in a lower-dimensional space.\n",
    "\n",
    "You can experiment with different values of n_components and n_clusters and see how they affect the results. You can also try other cross decomposition methods such as PLSRegression or PLSCanonical or other clustering methods such as DBSCAN or SpectralClustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfecb5cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

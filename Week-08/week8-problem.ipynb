{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdanny/COMP0189-practical/blob/main/Week-08/week8-problem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QvZjsta-6r8",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# COMP0189: Applied Artificial Intelligence\n",
        "## Week 8 (Reinforcement Learning)\n",
        "\n",
        "In this notebook you will learn how to implement from scratch the Q-Learning algorithm for deterministic Markov\n",
        "Decision Process and how to use the Deep Q-Learning algorithm using the TensorFlow-Agents library.\n",
        "\n",
        "Imagine we want to train an impostor from\n",
        "[Among Us](https://en.wikipedia.org/wiki/Among_Us) to make it capable of finding the closest vent to\n",
        "hide himself. To simplify this task we assume that the impostor moves in a grid world.\n",
        "\n",
        "We can model the impostor in this world as an agent that implements 4 actions,\n",
        "move up, move down, move right and move left,\n",
        "and we can model the environment as a grid world where the agent gets rewarded only once it has achieved its goal,\n",
        "it has found the vent.\n",
        "\n",
        "## The Environment\n",
        "\n",
        "If in supervised learning, we would usually spend most of our time collecting and cleaning the data, in RL this\n",
        "time is spent in designing the environment. Following we design a `GridWorld` class.\n",
        "\n",
        "### Acknowledgements\n",
        "This notebook is adapted from [CEGE0004: Machine Learning for Data Science](https://github.com/aldolipani/CEGE0004) (week 8 RL practical), written by [Dr. Aldo Lipani](https://aldolipani.com) (aldo.lipani@ucl.ac.uk), an Asst. Prof. in Machine Learning at the University College London (UCL)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lg0zeKaTABgu"
      },
      "outputs": [],
      "source": [
        "# Getting the Among Us images\n",
        "! wget https://raw.githubusercontent.com/kimdanny/COMP0189-practical/main/Week-08/imgs/agent.png\n",
        "! wget https://raw.githubusercontent.com/kimdanny/COMP0189-practical/main/Week-08/imgs/target.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TVlwZWe-6r-",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
        "\n",
        "class GridWold:\n",
        "\n",
        "    def __init__(self, n, init_pos=(0, 0), target_pos=None):\n",
        "        \"\"\"\n",
        "        A simple environment\n",
        "        :param n: The size of the world.\n",
        "        :param init_pos: The initial position of the agent. The agent start at the top-left corner by default.\n",
        "        :param target_pos: The position of the goal. The goals is set to the bottom-right corner by default.\n",
        "        \"\"\"\n",
        "        self.n = n\n",
        "        self.init_pos = init_pos\n",
        "        self.pos = init_pos\n",
        "        if target_pos is None:\n",
        "            self.target_pos = (n - 1, n - 1)\n",
        "        else:\n",
        "            self.target_pos = target_pos\n",
        "\n",
        "    def reset(self, pos=None):\n",
        "        \"\"\"\n",
        "        This method resets the environment.\n",
        "        :param pos: If we set the position, the agent will start not from the default position by from the\n",
        "        position set. This is useful when we will add some randomness in the training.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if pos is None:\n",
        "            self.pos = self.init_pos\n",
        "        else:\n",
        "            self.pos = pos\n",
        "        return self.pos\n",
        "\n",
        "    def get_available_actions(self):\n",
        "        \"\"\"\n",
        "        This method returns the set of available actions the agent can perform given its current position.\n",
        "        :return: The set of available actions.\n",
        "        \"\"\"\n",
        "        # if the agent has reached the vent, then no actions are available.\n",
        "        if self.pos == self.target_pos:\n",
        "            res = {}\n",
        "        else:\n",
        "            # all actions\n",
        "            res = {'up', 'down', 'right', 'left'}\n",
        "            # remove actions if the agent is at the edges of the world.\n",
        "            if self.pos[0] == 0:\n",
        "                res.discard('left')\n",
        "            elif self.pos[0] == self.n - 1:\n",
        "                res.discard('right')\n",
        "            if self.pos[1] == 0:\n",
        "                res.discard('up')\n",
        "            elif self.pos[1] == self.n - 1:\n",
        "                res.discard('down')\n",
        "        return res\n",
        "\n",
        "    def step(self, action: str):\n",
        "        \"\"\"\n",
        "        This method executes an action and changes the state of the agent. It returns the new position of the agent and\n",
        "        the reward received.\n",
        "        :param action: The action to execute.\n",
        "        :return: The position and reward.\n",
        "        \"\"\"\n",
        "        # executes an action\n",
        "        if action == 'left':\n",
        "            self.pos = (self.pos[0] - 1, self.pos[1])\n",
        "        elif action == 'right':\n",
        "            self.pos = (self.pos[0] + 1, self.pos[1])\n",
        "        elif action == 'down':\n",
        "            self.pos = (self.pos[0], self.pos[1] + 1)\n",
        "        elif action == 'up':\n",
        "            self.pos = (self.pos[0], self.pos[1] - 1)\n",
        "\n",
        "        # if the agent has achieved the goal, it returns a reward of 100, zero otherwise.\n",
        "        if self.pos == self.target_pos:\n",
        "            return self.pos, 100\n",
        "        else:\n",
        "            return self.pos, 0\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"\n",
        "        This method renders the environment and the agent.\n",
        "        \"\"\"\n",
        "        n = self.n\n",
        "        agent = OffsetImage(plt.imread('agent.png'), zoom=1 / (2 * n))\n",
        "        target = OffsetImage(plt.imread('target.png'), zoom=10 / (2 * n))\n",
        "\n",
        "        fig = plt.figure(figsize=(10, 10))\n",
        "        ax = fig.add_subplot(1, 1, 1)\n",
        "        for j in range(n + 1):\n",
        "            ax.axvline(x=j / n)\n",
        "            ax.axhline(y=j / n)\n",
        "\n",
        "        ab_target = AnnotationBbox(target,\n",
        "                                   xy=((self.target_pos[0] + 0.5) / n, 1 - (self.target_pos[1] + 0.7) / n),\n",
        "                                   frameon=False)\n",
        "        ax.add_artist(ab_target)\n",
        "\n",
        "        ab_agent = AnnotationBbox(agent,\n",
        "                                  xy=((self.pos[0] + 0.5) / n, 1 - (self.pos[1] + 0.5) / n),\n",
        "                                  frameon=False)\n",
        "        ax.add_artist(ab_agent)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl5gMfOz-6r_",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Let's now instantiate a gird-world of size 5 and render it. If we use the default parameters, you should see the\n",
        "impostor on the top-left corner, and the vent on the bottom-right corner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lL3x16qq-6sA",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "env = GridWold(5)\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TipWcjx-6sA",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## A Random Policy\n",
        "\n",
        "We will now simulate a random policy. An agent that navigates the world performing random actions. This agent will\n",
        "execute actions for a given number of times. While executing the actions\n",
        "we will also count the number of times the agent has achieved its goal (the number of episodes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0JGhA-PuQw-"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9zmj5pRuQw_"
      },
      "source": [
        "**Task 1. Run your random policy and render the image each step**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1ZRsxAm-6sB",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# the number of actions the agent will execute in total\n",
        "total_steps = 1000\n",
        "\n",
        "episodes = 0\n",
        "for n in tqdm(range(total_steps)):\n",
        "    # delay for display\n",
        "    time.sleep(0.1)\n",
        "    display.clear_output(wait=True)\n",
        "    available_actions = env.get_available_actions()\n",
        "    # Implement and run a random policy\n",
        "    ### YOUR CODE GOES BELOW ###\n",
        "    None\n",
        "    \n",
        "    env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTrG9ieZ-6sB",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Q-Learning\n",
        "\n",
        "We will now implement the Q-learning algorithm to learn the Q-values. These Q-values will be stored in a table called\n",
        "`q_table`. To pick the next action the agent will choose the action that maximizes the Q-value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCMb3NaOuQw_"
      },
      "source": [
        "**Task 2. Implement and train with Q-Learning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1T3SpMA-6sB",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# discount rate\n",
        "l = 0.9\n",
        "# the number of actions the agent will execute in total\n",
        "total_steps = 1000\n",
        "# q-values\n",
        "q_table = {}\n",
        "\n",
        "# reset env\n",
        "pos = env.reset()\n",
        "\n",
        "# render env\n",
        "env.render()\n",
        "time.sleep(0.1)\n",
        "display.clear_output(wait=True)\n",
        "\n",
        "episodes = 0\n",
        "for n in tqdm(range(total_steps)):\n",
        "    # delay for display\n",
        "    time.sleep(0.1)\n",
        "    display.clear_output(wait=True)\n",
        "    # get available actions\n",
        "    available_actions = env.get_available_actions()\n",
        "    # Implement and run Q-Learning\n",
        "    ### YOUR CODE GOES BELOW ###\n",
        "    None\n",
        "    \n",
        "    env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEOUKt1P-6sB",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "**Task 3. Now that we have trained the agent we can test it. Compare the performance of this agent against the random policy.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egKHze3B-6sB",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# the number of actions the agent will execute in total\n",
        "total_steps = 1000\n",
        "\n",
        "# reset env\n",
        "pos = env.reset()\n",
        "for n in tqdm(range(total_steps)):\n",
        "    time.sleep(0.1)\n",
        "    display.clear_output(wait=True)\n",
        "    # get available actions\n",
        "    available_actions = env.get_available_actions()\n",
        "    ### YOUR CODE GOES BELOW ###\n",
        "    None\n",
        "\n",
        "    env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJSS74L1-6sC",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## An example of Ready-Made Environments (MiniGrid)\n",
        "\n",
        "Here I provide you with an example of a ready-made environment where you can test RL algorithms.\n",
        "These environments are defined by the [MiniGrid](https://github.com/maximecb/gym-minigrid).\n",
        "\n",
        "Before executing the next code, please make sure that this package is correctly installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGvbVr6t-6sC",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "!pip install gym-minigrid==1.0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7heuyKTQSP0"
      },
      "outputs": [],
      "source": [
        "from gym_minigrid.wrappers import *\n",
        "\n",
        "env = gym.make('MiniGrid-Empty-16x16-v0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsaXx6T9-6sC"
      },
      "source": [
        "This is how the environment looks like at the beginning. We have a grid world with an agent in the top-left corner\n",
        "that can navigate this empty room. On the bottom-right corner we have the target location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRtZWV1E-6sD",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# You might have to run this cell twice if you face error.\n",
        "plt.figure(figsize=(9, 9))\n",
        "env.reset()\n",
        "plt.imshow(env.render(mode='rgb_array'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJfNqjhZ-6sD",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "We can access the actions available to the agent as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgnnS-xa-6sD",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "for action in list(env.actions):\n",
        "    print(action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rqw_YJuQ-6sD",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# You might have to run this cell twice if you face error.\n",
        "\n",
        "from tqdm import tqdm\n",
        "total_steps = 100\n",
        "\n",
        "for n in tqdm(range(total_steps)):\n",
        "    time.sleep(0.1)\n",
        "    display.clear_output(wait=True)\n",
        "    plt.imshow(env.render(mode='rgb_array'))\n",
        "    plt.show()\n",
        "    action = env.action_space.sample()\n",
        "    _, reward, _, _ = env.step(action)\n",
        "\n",
        "    print(f'Iteration: {n + 1} of Episodes {episodes}')\n",
        "    print(f'Action taken: {action}')\n",
        "    print(f'Reward: {reward}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwEe__cC-6sD",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Deep Q-Learning with TF-Agents\n",
        "\n",
        "This example shows how to train a\n",
        "[DQN (Deep Q Networks)](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)\n",
        "agent using the TF-Agents library.\n",
        "\n",
        "Before executing the next code, please make sure that these packages are correctly installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93yRJuRC-6sE",
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow\n",
        "!pip install tf-agents==0.8.0\n",
        "!pip install pyglet==1.5.11"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ4c_W-a-6sE",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### The Cartpole Environment\n",
        "\n",
        "We will train a DQN agent for the Cartpole environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRPGQbHYSHpm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-JySmE1tHg7"
      },
      "outputs": [],
      "source": [
        "!pip install gym[classic_control]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_D-dfWaB-6sE",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from tf_agents.environments import suite_gym, tf_py_environment\n",
        "\n",
        "env = suite_gym.load('CartPole-v1')\n",
        "env = tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "env.reset()\n",
        "env.render()\n",
        "\n",
        "print('Observation Spec:', env.time_step_spec().observation)\n",
        "print('Reward Spec:', env.time_step_spec().reward)\n",
        "print('Action Spec:', env.action_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hYbgxLx-6sE",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Let's now try executing always one action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emFYOKhF-6sE",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "time_step = env.reset()\n",
        "print('Time step:')\n",
        "print(time_step)\n",
        "\n",
        "while not time_step.is_last():\n",
        "    action = np.array(1, dtype=np.int32)\n",
        "    time_step = env.step(action)\n",
        "    env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUKHOO3E-6sF",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "We can see that by doing this the episode ends quickly. This naive agent fails quickly.\n",
        "\n",
        "### The DQN Agent\n",
        "\n",
        "The DQN agent can be used in any environment which has a discrete action space.\n",
        "At the heart of a DQN Agent is a QNetwork, a neural network model that can learn to predict\n",
        "Q-Values (expected returns) for all actions, given an observation from the environment.\n",
        "\n",
        "The QNetwork consists of a sequence of Dense layers, where the final layer\n",
        "will have 1 output for each possible action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOtU4a6C-6sF",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.specs import tensor_spec\n",
        "\n",
        "fc_layer_params = (100, 50)\n",
        "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
        "\n",
        "\n",
        "# Define a helper function to create Dense layers configured with the right\n",
        "# activation and kernel initializer.\n",
        "def dense_layer(num_units):\n",
        "    return tf.keras.layers.Dense(\n",
        "        num_units,\n",
        "        activation=tf.keras.activations.relu,\n",
        "        kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
        "            scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
        "\n",
        "\n",
        "# QNetwork consists of a sequence of Dense layers followed by a dense layer with `num_actions` units to generate one\n",
        "# q_value per available action as it's output.\n",
        "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
        "q_values_layer = tf.keras.layers.Dense(\n",
        "    num_actions,\n",
        "    activation=None,\n",
        "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
        "        minval=-0.03, maxval=0.03),\n",
        "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
        "q_net = sequential.Sequential(dense_layers + [q_values_layer])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRnZjC4K-6sF",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "**Task 4. Now we instantiate a `DqnAgent`. The agent constructor also requires an\n",
        "optimizer, a loss function, and an integer step counter.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJl-JvBHuQxF"
      },
      "outputs": [],
      "source": [
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.utils import common"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UPoOLTT-6sG",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "optimizer = None ### YOUR CODE GOES LEFT ###\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    ### YOUR CODE GOES BELOW ###\n",
        "    None\n",
        ")\n",
        "\n",
        "agent.initialize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVYcy7h--6sG",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Before training the DQN agent we can test how a random policy would behave in this environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Wo0tqa0-6sH",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from tf_agents.policies import random_tf_policy\n",
        "\n",
        "random_policy = random_tf_policy.RandomTFPolicy(env.time_step_spec(), env.action_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNshw6it-6sH",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "We also create a helper function that will allow us render any policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJ75MuRc-6sH",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def render_policy(policy):\n",
        "    time_step = env.reset()\n",
        "    env.render()\n",
        "    while not time_step.is_last():\n",
        "        action_step = policy.action(time_step)\n",
        "        time_step = env.step(action_step.action)\n",
        "        env.render()\n",
        "\n",
        "render_policy(random_policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHS1F_Th-6sI",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Given a policy we are interested in evaluating what is its average return.\n",
        "To do that we run the policy for multiple episodes and average their returns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Gd3T_kR-6sI",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "    res = 0.0\n",
        "    for _ in range(num_episodes):\n",
        "        time_step = environment.reset()\n",
        "        episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "        action_step = policy.action(time_step)\n",
        "        time_step = environment.step(action_step.action)\n",
        "        episode_return += time_step.reward\n",
        "    res += episode_return\n",
        "\n",
        "    avg_return = res / num_episodes\n",
        "    return avg_return.numpy()[0]\n",
        "\n",
        "print('Average return of the random policy:', compute_avg_return(env, random_policy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el2bk1qt-6sJ",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Replay Buffer\n",
        "\n",
        "The replay buffer keeps track of data collected from the environment.\n",
        "This allows the agent to train on previous experiences. This helps in stabilizing the training.\n",
        "We use the `TFUniformReplayBuffer`, as it is the most common.\n",
        "\n",
        "Refer to: https://paperswithcode.com/method/experience-replay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSvCgrOe-6sK",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "\n",
        "replay_buffer_max_length = 100000\n",
        "\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=env.batch_size,\n",
        "    max_length=replay_buffer_max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhzdoi1I-6sK",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "We can now execute the random policy in the environment for a few steps, and\n",
        "record the data in the replay buffer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DiM7zdz-6sK",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from tf_agents.trajectories import trajectory\n",
        "\n",
        "def collect_step(environment, policy, buffer):\n",
        "    time_step = environment.current_time_step()\n",
        "    action_step = policy.action(time_step)\n",
        "    next_time_step = environment.step(action_step.action)\n",
        "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "\n",
        "    # add trajectory to the replay buffer\n",
        "    buffer.add_batch(traj)\n",
        "\n",
        "def collect_data(env, policy, buffer, steps):\n",
        "    for _ in range(steps):\n",
        "        collect_step(env, policy, buffer)\n",
        "\n",
        "initial_collect_steps = 100\n",
        "collect_data(env, random_policy, replay_buffer, initial_collect_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsbfzJyk-6sK",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Training the DQN Agent\n",
        "\n",
        "**Task 5. Implement a DQN Agent training.  \n",
        "Two things must happen during the training loop:**\n",
        "-   collect data from the environment\n",
        "-   use that data to train the agent's neural network(s)\n",
        "-   periodically evaluates the policy and prints the current score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84cJTfy9-6sL",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "### YOUR CODE GOES BELOW ###\n",
        "None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYzgcp2p-6sL",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "**Task 6. See how the training progressed by plotting the average return over the iterations.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3mJ2wl1uQxI"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE GOES BELOW ###\n",
        "import matplotlib.pyplot as plt\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')\n",
        "None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHUVfZlA-6sL",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "**Task 7. Now see how the agent behaves.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oH-M8Z2n-6sM",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "### YOUR CODE GOES BELOW ###\n",
        "None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4n11jwE-6sM",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2Pz1r3ii5Qs"
      },
      "source": [
        "## The Same Cartpole Example in Pytorch\n",
        "\n",
        "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        "\n",
        "[Run in Colab](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/9da0471a9eeb2351a488cd4b44fc6bbf/reinforcement_q_learning.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltmdbtUnj4SC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}